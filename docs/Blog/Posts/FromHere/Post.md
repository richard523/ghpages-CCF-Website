![alt text](Assets/road-motorcycle.jpg)


# Where We Go From Here
## An Introduction to the Carboncopies Foundation 2019 Summer Event "Updating the Roadmap to Whole Brain Emulation Part 2: Where We Go From Here."
## by Dr. Randal Koene

Whole brain emulation is the technical term for a process by which the very specific functions of a brain can be recreated so that a mind with particular characteristics, memories and experiences is produced. It is the process for mind uploading.

In all likelihood, whole brain emulation is physically possible, though very difficult by today’s standards.

The beginnings of what will be involved to achieve a whole brain emulation are visible today in the efforts of neuroscientists. Indeed, we could say that whole brain emulation should be a natural outcome of successful neuroscientific study as the domain of neuroscience matures toward a complete understanding and an ability to computationally model brain mechanisms.

Consequently, my response when asked about the timeline for whole brain emulation is often this: I would be extremely surprised if it was accomplished within 20 years, but I would be equally astonished if it did not happen within 100 years, assuming that infrastructure, economy and science continue as they have.

By historic, evolutionary and certainly by astronomical standards, 100 years is not even a blink of the eye. In other words, if we assume that humanity continues to thrive and strive, then we can also assume that humanity is right now on the very cusp of achieving whole brain emulation, and by extension, mind uploading.

Just a little over 10 years ago, in 2008, the first attempted roadmap toward whole brain emulation was published by the Future of Humanity Institute at Oxford University. I was involved in that publication, which featured the collected and edited output of a group of researchers who convened at Oxford in 2007 for the first workshop on whole brain emulation.

The first roadmap was necessarily incomplete at the time; and it focused heavily on a subset of the problems of whole brain emulation. It is time to compile the knowledge and the data for the updated version of the roadmap. To that end, on June 16th of this year, the Carboncopies Foundation launched a series of events that are aimed at presenting updated accomplishments and updated understanding to the public in preparation for the new version.

In our first event of the series, Prof. Choe (Texas A&M) gave us a peek at the scientific process for a better understanding of the phenomenon of consciousness, by devising models of neural dynamics that can predict aspects of consciousness. Prof. Song (USC) revisited patient-specific prosthetic neural model building and advances of memory decoding experiments with the neuromimetic hippocampal prosthesis. Prof. Parker (USC) reminded us of the importance to test the functional relevance of characteristics of biological neural signaling. Dr. Deca (NeurobotX) presented grid cell responses as an example where evolved coding (in this case for space) may enable superior autonomous navigation. She posed this as an example of the interplay between emulation of brain mechanisms and insights for A.I. Dr. Hayworth revisited the state of the art in brain preservation, which has reached a point where reliable long-term brain preservation appears to be both possible and practical. Hayworth also emphasized the importance of WBE proof-of-concept studies and the overall need to build consensus around the connection between fundamental neuroscience research and logical outcomes with meaningful impact in the world. Prominent neuroscientists must take responsibility for this reality.

The panel discussion then turned to the obvious question: Where do we go from here? Creating working models from extracted data, and validating results according to improved success criteria were foremost on the list.

The second event of the series is scheduled for September 21st. It is titled, “Updating the Roadmap to Whole Brain Emulation, Part 2: Where We Go From Here.” It asks the question: Imagine it is the year 2030 and we can collect all of the brain data that we expect to be able to collect – how do you use that to do whole brain emulation?

The event will feature interviews and panel discussions, and we will explicitly ask the participants to take that step, to look beyond the predictable next engineering step and instead at the more significant problems that still need to be solved. So that we may separate philosophy from physics, we will address this for a tiny animal first, before we ask about human mind uploading.

In the June 16th event, we already learned that aldehyde stabilized cryopreservation may be the solution for the long-term storage of brain tissue in a manner that preserves every relevant detail of the structure of the brain’s connectome. We look at that, and we say brain preservation is scientifically a solved matter. Turning it into a reliable process that can be used for human-size brains is an engineering problem that will make almost predictable strides in the coming years. We can write about those details, the milestones that are the peaks and valleys in the roadmap, but we don’t need to worry much about discovering a path.

We also learned about multi-beam and focused ion beam electron microscopy technologies that are multiplying the volume of brain tissue that can be accurately scanned in a reasonable amount of time at resolutions that show the details of the brain’s connectome which our consensus assumptions about whole brain emulation deem necessary. Certainly, there are still some questions about reliably identifying what has been scanned, or about methods to label what will be scanned via chemical or molecular means for such identification. But the capacity increases, that are needed to get to the point where imaging the connectome of brain tissue is possible at the scale of a human brain, are looking more and more like engineering challenges instead of fundamental scientific questions. Again, we can write a lot about those details, but we are less worried about discovering a feasible path than we might have been 10 years ago.

The first roadmap didn’t say that much about electrophysiology or about collecting electrophysiological recordings from a very large number of sites throughout a living brain. That is a type of data collection that is crucially important to the characterization of brain mechanisms and systems. The road there still looks a little murkier, despite the efforts of the Human Brain Project, the Brain Initiative, the Allen Brain Institute, and many others. Multi-site recording is beginning to take off, but its ultimate potential, and the optimal technology path, is still a little uncertain. There may be several paths worth taking; therefore, there is some terrain the roadmap should suggest to explore.

Notice that I haven’t mentioned any models yet, any emulated brains or emulated pieces of brain. Those models are where it all has to be put together. In previous events, the Carboncopies Foundation has frequently highlighted the efforts by the Berger lab and its associates to create a neuroprosthesis for regions of the human hippocampus. We did that, specifically, to highlight this important problem of bringing the structure, the characterized functions, and the desired functional output together in one model, in one emulation.

The hippocampal prosthesis project is an attempt to create such models, such emulations of a specific patient’s brain function, and then to cast those into medical devices tailored specifically to that patient’s needs. The results are preliminary though promising. The chosen system, in regions CA3 and CA1 of the hippocampus, is purposely structurally relatively simple. It means that researchers working on that prosthesis were able to focus on model building by combining well understood structure with characteristics of recorded data. That hasn’t been possible in most other parts of the brain, where connectome details still need more data and more effort to reconstruct. The retina of the eye is one other relatively simple region where prosthetic modeling has been possible. So, this is where we begin to run into a multitude of scientific and applied science questions. There are many possible paths to follow and probably many studies to conduct.

We would like the updated roadmap to describe not just the terrain historically covered, or the known engineering paths with predictions for the time from capability A to capability B. Rather, we would like the roadmap to look ahead to our goal. We would like it to include the best ideas for exploration, and so we would like to ask the pioneers to consider the unknown pieces.

Dear Pioneer, come with us into our virtual landscape. Imagine that you find yourself at the end of the path that has been taken so far, and even a little further – as far as we can comfortably see along the path. So, please imagine that it is 2030, and we have reached the point where brains can be preserved virtually forever, without any damage to connectome relevant matter. Please also imagine that scanning of brain tissue at Electron Microscope scale has reached the point where the details of entire brains can be digitized in reasonable time and at reasonable cost. Perhaps these are Drosophila brains in 2030. That’s fine. Let’s also assume that labels and other identification methods allow us to automatically process the image data into 3-dimensional maps that show the locations of, the spatial dimensions of, and the connections between all of the identified scan content. Imagine therefore, that we have in 2030 (or is it 2040?) a process that can convert the preserved tissue of a brain into a reasonably reliable and accurate full connectome, synaptome, etcetera-tome of that brain.

Let’s first assume that we want to create a working whole brain emulation of a Drosophila with its 135 thousand neurons. Where would you take us next on our exploratory journey to this goal?

Keep in mind that the maps created with the data that was acquired are not working models. They are a collection of data without processes to run. They are the image of the sea, not the sea roiling under the influence of gravitation and winds.

Perhaps, first it would be a good exercise to know a little bit more about the goal. For our whole Drosophila-brain emulation, what are the success criteria that we use to determine if a successful emulation was built?

Is there a systematic process that you can imagine – and which we can then discuss in detail for the updated roadmap – by which all of those brain tissue maps are converted into processes, including the constraints and means of validation that are needed to determine that the resulting processes are not just any processes, but are indeed those that generate a successful whole brain emulation of that specific Drosophila?

What are the main difficulties you can imagine? For example. are you at all worried about justified scale separation in a complex system? Are there important alternate routes to consider that you can already see in the distance?

That was quite the hike! Awesome.

Now, for the next step, let’s assume that we want to create a working whole brain emulation to upload your mind.

Are there any new success criteria that need to be met? And do those raise any additional difficulties that we should discuss before we embark on that journey?

Thank you. We’ll be poring over this wonderful travel log, and then we’ll be back with our next set of questions as we try to collect what we need to know for that updated roadmap. Your travels will tell us a lot about the updated state of what we might call the consensus approach to whole brain emulation:

- Several parts of the approach will have moved from scientific problems to being largely engineering problems that present a clear progress curve towards the required capabilities on a somewhat predictable timeline.
- Others will still contain basic scientific questions, even though the consensus about underlying assumptions is still strong. For example, that cognitive functions are accomplished by neurons that interact through the exchange of electrochemical signals, and that a separation of scales is possible whereby some aspects of the biological machinery behind that interaction are not in themselves crucial to the cognitive functions experienced.
- Finally, there may be some areas where our understanding has changed in important ways over the last decade, with new findings and insights that challenge specific assumptions of the consensus approach.

